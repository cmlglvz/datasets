---
title: "DADA2"
output: html_document
---

## Libraries

```{r echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
library(ShortRead)
library(tidyverse)
library(dada2)
library(phyloseq)
library(ape)
library(Biostrings)
```


## Import sequences for quality profile analyzes

Define the following path variable so that it points to the extracted directory on your machine. Then we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
path <- "E:/R/Proyecto_Doctorado/Objetivo_1/Nuevo_Orden/Todas/" # 19 samples
fns <- list.files(path)
fastqs <- fns[grepl(".fastq.gz", fns)] 
fastqs <- sort(fastqs)
fnFs <- fastqs[grepl("_L001_R1.fastq.gz", fastqs)] # Fws
fnRs <- fastqs[grepl("_L001_R2.fastq.gz", fastqs)] # Rvs

# Get sample names from the first part of the forward read filenames
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# Para especificar de manera concreta donde se encuentras las secuencias Fw y Rv
fnFs <- paste0(path, fnFs)
fnRs <- paste0(path, fnRs)
```

Finally, we visualize the quality profiles of the reads:
In **gray-scale** is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the **green line**, and the quartiles of the quality score distribution by the **orange lines**.

#### Quality profiles visualization
```{r, echo=TRUE, message=FALSE, warning=FALSE}
plotQualityProfile(fnFs)
plotQualityProfile(fnRs)
```

The **forward reads** are generally of good quality. It is advise to trim the last few nucleotides to avoid less well-controlled errors that can arise there.

The **reverse reads** are of significantly worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as **DADA2** incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities (for all sequences) crash will improve the algorithm’s sensitivity to rare sequence variants.


## Sequence trimming

Based on quality profiles we can trim **NTs** from left (*trimLeft*) and anything below **Q30** (*truncLen*) on the right. Also we can set filtering parameters according to our needs (computing power, strictness and so on).

```{r, echo=TRUE, message=TRUE, warning=TRUE}
ptm <- proc.time() # Como método de control del proceso
filtpth <- file.path(path, "Filtered_")
filtFs <- paste0(filtpth, sample.names, "_F_filt.fastq.gz")
filtRs <- paste0(filtpth, sample.names, "_R_filt.fastq.gz")

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(240,200), trimLeft=c(15,15), 
                     maxN=0, maxEE=c(2,2), truncQ=2, 
                     compress=TRUE, multithread=FALSE, verbose=TRUE)
# En Windows configurar multithread=FALSE

knitr::kable(out)

proc.time() - ptm
```


## Error rate estimation

The DADA2 algorithm makes use of a parametric error model and every amplicon dataset has a different set of error rates. This method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r, echo=TRUE, message=FALSE, warning=FALSE}
ptm <- proc.time()
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
proc.time() - ptm
```

It is always worthwhile to visualize the estimated error rates:

#### Error rate graphs.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```
The error rates for each possible transition (A→C, A→G, …) are shown. **Points** are the observed error rates for each consensus quality score. The **black line** shows the estimated error rates after convergence of the machine-learning algorithm. The **red line** shows the error rates expected under the nominal definition of the Q-score.


## Dereplicate

Finding the set of unique sequences, equivalently, the process of finding duplicated (replicate) sequences. 

```{r,echo=TRUE, message=FALSE, warning=FALSE}
ptm <- proc.time()
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names # Para que los objetos derreplicados correspondan a los de las lecturas filtradas
names(derepRs) <- sample.names
proc.time() - ptm
```


## Joint sample inference and error rate estimation.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
ptm <- proc.time()
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
proc.time() - ptm
```

By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. The dada2 package offers two types of pooling. *pool=TRUE* performs standard pooled processing, in which all samples are pooled together for sample inference. *pool="pseudo"* performs pseudo-pooling, in which samples are processed independently after sharing information between samples, approximating pooled sample inference in linear time.


## Merge paired reads.

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (these conditions can be changed via function arguments). Non-overlapping reads are supported, but not recommended

```{r, echo=TRUE, message=FALSE, warning=FALSE}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
```


## Construct ASV table

The sequence table is a *matrix* with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab <- makeSequenceTable(mergers)
table(nchar(getSequences(seqtab))) # Inspect distribution of sequence lengths
```


## Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

The frequency of chimeric sequences varies substantially from dataset to dataset, and depends on factors including experimental procedures and sample complexity.

Most of your **reads** should remain after chimera removal (it is not uncommon for a majority of **sequence variants** to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to beginning the DADA2 pipeline.

By default the *method* is set in **"consensus"** where the samples in a sequence table are independently checked for bimeras, and a **consensus decision on each sequence variant is made**. If it sets as **"per-sample"**, samples in a sequence table are independently checked for bimeras, **and sequence variants are removed (zeroed-out) from samples independently**. Another alternative is **"pooled"** the samples fot bimera identification.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", verbose = TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```


## Track reads through the pipeline

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r, echo=TRUE, message=FALSE, warning=FALSE}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- sample.names
print(track)
#If error in some point, go back to those and roll again
```

Outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the *truncLen* parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.

#### Remove sequence variants seen less than a given number of times 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab.nochim = seqtab.nochim[,colSums(seqtab.nochim) > 10]
# Opcional, de acuerdo a tus criterios y teniendo en cuenta el impacto que tendrán en tus resultados (el porqué)
```


## Write sequence table to file
```{r, echo=TRUE, message=FALSE, warning=FALSE}
write.csv2(seqtab.nochim, file = "E:/R/Proyecto_Doctorado/Objetivo_1/Nuevo_Orden/Analisis/Todas/seqtab_nochim.csv")
```


## Taxonomy assignation (Silva, RDP, Greengenes, etc.)

It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The *assignTaxonomy* function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least *minBoot* bootstrap confidence.

```{r Taxonomy assignation, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
taxa_v14 <- assignTaxonomy(seqtab.nochim, "E:/R/DBs/pr2_version_4.14.0_SSU_dada2.fasta.gz", taxLevels = c("Kingdom", "Supergroup", "Division", "Class", "Order", "Family", "Genus", "Species"), multithread = 12)
```


```{r Inspection, echo=TRUE,message=FALSE, warning=FALSE, include=TRUE}
# Removing sequence rownames for display only
strict.14p <- taxa_v14
rownames(strict.14p) <- NULL
head(strict.14p)
```


## Write taxonomy assignments to file (PR2)

### PR2 4.14.0
```{r Taxonomy table, message=FALSE, warning=FALSE, include=TRUE}
write.csv2(taxa_v14, file = "E:/R/Proyecto_Doctorado/Objetivo_1/Nuevo_Orden/Analisis/Todas/Taxa_PR2.csv")
```



### END
